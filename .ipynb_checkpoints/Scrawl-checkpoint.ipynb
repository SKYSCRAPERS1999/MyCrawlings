{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-04T11:20:37.721918Z",
     "start_time": "2018-08-04T11:20:37.709167Z"
    }
   },
   "outputs": [],
   "source": [
    "def request_view(response):\n",
    "    import webbrowser\n",
    "    request_url = response.url\n",
    "    base_url = '<head><base href=\"%s\">'%(request_url)\n",
    "    base_url = base_url.encode()\n",
    "    content = response.content.replace(b\"<head>\", base_url)\n",
    "    tem_html = open(\"tmp.html\", \"wb\")\n",
    "    tem_html.write(content)\n",
    "    tem_html.close()\n",
    "    webbrowser.open_new_tab(\"tmp.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "response = requests.get(\"http://m.qiubaichengren.net/\")\n",
    "request_view(response)\n",
    "# print (response.status_code, response.content.decode())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lxml import etree\n",
    "html = etree.HTML(response.content)\n",
    "# html.xpath(\"//div[@class='page']//a[contains(text(),'下一页')]/@href\")\n",
    "html.xpath(\"//li[@class='next']/i/a/@href\")\n",
    "# from bs4 import BeautifulSoup\n",
    "# soup = BeautifulSoup(response.content, 'html.parser')\n",
    "# print(soup.prettify())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from time import sleep\n",
    "host = \"http://m.qiubaichengren.net/\"\n",
    "def test_all_links(url):\n",
    "    response = requests.get(url)\n",
    "    print(url, response.status_code)\n",
    "    from lxml import etree\n",
    "    html = etree.HTML(response.content)\n",
    "    links = html.xpath(\"//li[@class='next']/i/a/@href\")\n",
    "    if len(links) == 0:\n",
    "        pass\n",
    "    else:\n",
    "        sleep(1)\n",
    "        test_all_links(host + links[0])\n",
    "        \n",
    "test_all_links(\"http://m.qiubaichengren.net/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from lxml import etree\n",
    "\n",
    "class Login(object):\n",
    "    def __init__(self):\n",
    "        self.headers = {\n",
    "            'Referer': 'https://github.com/',\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/57.0.2987.133 Safari/537.36',\n",
    "            'Host': 'github.com'\n",
    "        }\n",
    "        self.login_url = 'https://github.com/login'\n",
    "        self.post_url = 'https://github.com/session'\n",
    "        self.logined_url = 'https://github.com/settings/profile'\n",
    "        self.session = requests.Session()\n",
    "\n",
    "    def token(self):\n",
    "        response = self.session.get(self.login_url, headers = self.headers)\n",
    "        selector = etree.HTML(response.text)\n",
    "        token = selector.xpath('//*[@id=\"login\"]/form/input[2]/@value')\n",
    "        return token\n",
    "    \n",
    "    def login(self, email, password):\n",
    "        post_data = {\n",
    "            'commit': 'Sign in',\n",
    "            'utf8': '✓',\n",
    "            'authenticity_token': self.token(),\n",
    "            'login': email,\n",
    "            'password': password\n",
    "        }\n",
    "        response = self.session.post(self.post_url, data=post_data, headers=self.headers)\n",
    "        print (response.status_code)\n",
    "        if response.status_code == 200:\n",
    "            print ('ok1')\n",
    "            self.dynamics(response.text)\n",
    "        \n",
    "        response = self.session.get(self.logined_url, headers=self.headers)\n",
    "        if response.status_code == 200:\n",
    "            print ('ok2')\n",
    "            self.profile(response.text)\n",
    "            \n",
    "    def dynamics(self, html):\n",
    "        selector = etree.HTML(html)\n",
    "        dynamics = selector.xpath('//div[contains(@class, \"news\")]//div[contains(@class, \"alert\")]')\n",
    "        for item in dynamics:\n",
    "            dynamic = ' '.join(item.xpath('.//div[@class=\"title\"]//text()')).strip()\n",
    "            print(dynamic)\n",
    "    \n",
    "    def profile(self, html):\n",
    "        selector = etree.HTML(html)\n",
    "        name = selector.xpath('//input[@id=\"user_profile_name\"]/@value')\n",
    "        email = selector.xpath('//select[@id=\"user_profile_email\"]/option[@value!=\"\"]/text()')\n",
    "        print(name, email)\n",
    "        \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    login = Login()\n",
    "    login.login(email='386333667@qq.com', password='zhzh995=')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from lxml import etree\n",
    "\n",
    "class Login(object):\n",
    "    def __init__(self):\n",
    "        self.headers = {\n",
    "            'Referer': 'https://github.com/',\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/57.0.2987.133 Safari/537.36',\n",
    "            'Host': 'github.com'\n",
    "        }\n",
    "        self.login_url = 'https://github.com/login'\n",
    "        self.post_url = 'https://github.com/session'\n",
    "        self.logined_url = 'https://github.com/settings/profile'\n",
    "        self.session = requests.Session()\n",
    "    \n",
    "    def token(self):\n",
    "        response = self.session.get(self.login_url, headers=self.headers)\n",
    "        selector = etree.HTML(response.text)\n",
    "        token = selector.xpath('//div//input[2]/@value')\n",
    "        return token\n",
    "    \n",
    "    def login(self, email, password):\n",
    "        post_data = {\n",
    "            'commit': 'Sign in',\n",
    "            'utf8': '✓',\n",
    "            'authenticity_token': self.token()[0],\n",
    "            'login': email,\n",
    "            'password': password\n",
    "        }\n",
    "        response = self.session.post(self.post_url, data=post_data, headers=self.headers)\n",
    "        if response.status_code == 200:\n",
    "            self.dynamics(response.text)\n",
    "        \n",
    "        response = self.session.get(self.logined_url, headers=self.headers)\n",
    "        if response.status_code == 200:\n",
    "            self.profile(response.text)\n",
    "    \n",
    "    def dynamics(self, html):\n",
    "        selector = etree.HTML(html)\n",
    "        dynamics = selector.xpath('//div[contains(@class, \"news\")]//div[contains(@class, \"alert\")]')\n",
    "        for item in dynamics:\n",
    "            dynamic = ' '.join(item.xpath('.//div[@class=\"title\"]//text()')).strip()\n",
    "            print(dynamic)\n",
    "    \n",
    "    def profile(self, html):\n",
    "        selector = etree.HTML(html)\n",
    "        name = selector.xpath('//input[@id=\"user_profile_name\"]/@value')[0]\n",
    "        email = selector.xpath('//select[@id=\"user_profile_email\"]/option[@value!=\"\"]/text()')\n",
    "        print(name, email)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    login = Login()\n",
    "    login.login(email='386333667@qq.com', password='zhzh995=')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "browser = webdriver.Chrome()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-31T03:09:54.134701Z",
     "start_time": "2018-07-31T03:09:54.082773Z"
    }
   },
   "outputs": [],
   "source": [
    "import math\n",
    "math.isnan('nan')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-31T04:02:51.076181Z",
     "start_time": "2018-07-31T04:02:51.073092Z"
    }
   },
   "outputs": [],
   "source": [
    "print(float(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-01T05:17:07.147181Z",
     "start_time": "2018-08-01T05:17:07.135631Z"
    }
   },
   "outputs": [],
   "source": [
    "with open('Documents/Codeforces/Muli4/output.txt') as fp:\n",
    "    for expr in fp:\n",
    "        if (expr != 'IMPOSSIBLE' and expr != None):\n",
    "            x = eval(expr)\n",
    "            print (x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-02T04:34:17.059260Z",
     "start_time": "2018-08-02T04:34:17.049010Z"
    }
   },
   "outputs": [],
   "source": [
    "ans = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-08-02T04:34:30.582Z"
    }
   },
   "outputs": [],
   "source": [
    "for i in range(100000):\n",
    "    for j in range(i*i+1, (i+1)*(i+1)):\n",
    "        if (i*i*i*i % j == 0):\n",
    "            ans.append(i)\n",
    "            break\n",
    "print (ans)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Goverment Datas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-06T17:24:09.671210Z",
     "start_time": "2018-08-06T17:24:09.496436Z"
    }
   },
   "outputs": [],
   "source": [
    "from lxml import etree\n",
    "import requests, re, time\n",
    "from bs4 import BeautifulSoup\n",
    "headers = {\n",
    "    'User-Agent':'Mozilla/5.0 (Windows; U; Windows NT 6.1; en-US; rv:1.9.1.6) Gecko/20091201 Firefox/3.5.6'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-03T03:39:31.427400Z",
     "start_time": "2018-08-03T03:39:31.421117Z"
    }
   },
   "source": [
    "## ShangWuBu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### China"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-03T17:32:30.977331Z",
     "start_time": "2018-08-03T17:32:30.962305Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_swb_china():\n",
    "    print ('In {}, begin, date is {}'.format(get_swb_china.__name__, time.strftime('%Y-%m-%d-%H:%M', time.localtime(time.time()))))\n",
    "        \n",
    "    base_url = 'http://www.mofcom.gov.cn'\n",
    "    response = requests.get(base_url + '/article/zhengcejd/', headers=headers)\n",
    "    if (response.status_code != 200):\n",
    "        return None\n",
    "    text = str(response.content, 'utf-8')\n",
    "    html = etree.HTML(text)\n",
    "    xpath = '//*[@id=\"wrap\"]/div[2]/div/div[1]/div[2]/ul/li/a[1]/text()'\n",
    "    titles = html.xpath(xpath)\n",
    "    # print (titles)\n",
    "    xpath = '//*[@id=\"wrap\"]/div[2]/div/div[1]/div[2]/ul/li/a[1]/@href'\n",
    "    links = html.xpath(xpath)\n",
    "    # print (links)\n",
    "    xpath = '//*[@id=\"wrap\"]/div[2]/div/div[1]/div[2]/ul/li/span/text()'\n",
    "    dates = html.xpath(xpath)\n",
    "    # print (dates)\n",
    "    data_china = [ (t, l, d, 'china') for (t, l, d) in zip(titles, links, dates) ]\n",
    "\n",
    "    response = requests.get(base_url + '/article/b/', headers=headers)\n",
    "    if (response.status_code != 200):\n",
    "        return None\n",
    "    text = str(response.content, 'utf-8')\n",
    "    html = etree.HTML(text)\n",
    "    xpath = '//*[@id=\"wrap\"]/div[2]/div/div[1]/div[2]/ul/li/a/text()'\n",
    "    titles = html.xpath(xpath)\n",
    "    xpath = '//*[@id=\"wrap\"]/div[2]/div/div[1]/div[2]/ul/li/a/@href'\n",
    "    links = html.xpath(xpath)\n",
    "    links = [ base_url + str(x) for x in links]\n",
    "    xpath = '//*[@id=\"wrap\"]/div[2]/div/div[1]/div[2]/ul/li/span/text()'\n",
    "    dates = html.xpath(xpath)\n",
    "\n",
    "    data_china = data_china + [ (t, l, d, 'china') for t, l, d in zip(titles, links, dates) ]\n",
    "\n",
    "    dict_china = [{'pos':x[3], 'title': x[0], 'link': x[1], 'date': x[2]} for x in data_china]\n",
    "  \n",
    "    print (len(dict_china))\n",
    "\n",
    "    print ('In {}, end, date is {}'.format(get_swb_china.__name__, time.strftime('%Y-%m-%d-%H:%M', time.localtime(time.time()))))\n",
    "\n",
    "    return dict_china"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-03T03:39:51.363515Z",
     "start_time": "2018-08-03T03:39:51.354055Z"
    }
   },
   "source": [
    "### JiangSu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-03T17:32:20.833790Z",
     "start_time": "2018-08-03T17:32:20.820300Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_swb_jiangsu():\n",
    "    \n",
    "    print ('In {}, begin, date is {}'.format(get_swb_jiangsu.__name__, time.strftime('%Y-%m-%d-%H:%M', time.localtime(time.time()))))\n",
    "\n",
    "    base_url = 'http://swt.jiangsu.gov.cn'\n",
    "    response = requests.get(base_url + '/col/col12660/index.html', headers=headers)\n",
    "    if (response.status_code != 200):\n",
    "        return None\n",
    "    text = str(response.content, 'utf-8')\n",
    "    # print (text)\n",
    "    regex = re.compile('<a href=\"(.*)\" target=\"_blank\">(.*)</a><span style=\".*\"> \\((.*)\\)</span>')\n",
    "\n",
    "    data_jiangsu = [(x[1], base_url+x[0], x[2].replace('/', '-'), 'jiangsu') for x in re.findall(pattern=regex, string=text)]\n",
    "    \n",
    "    dict_jiangsu = [{'pos':x[3], 'title': x[0], 'link': x[1], 'date': x[2]} for x in data_jiangsu]\n",
    "    \n",
    "    print (len(dict_jiangsu))\n",
    "    \n",
    "    print ('In {}, end, date is {}'.format(get_swb_jiangsu.__name__, time.strftime('%Y-%m-%d-%H:%M', time.localtime(time.time()))))\n",
    "\n",
    "    return dict_jiangsu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ZheJiang"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-03T17:32:15.104636Z",
     "start_time": "2018-08-03T17:32:15.089498Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_swb_zhejiang():\n",
    "    \n",
    "    print ('In {}, begin, date is {}'.format(get_swb_zhejiang.__name__, time.strftime('%Y-%m-%d-%H:%M', time.localtime(time.time()))))\n",
    "\n",
    "    base_url = 'http://zhejiang.mofcom.gov.cn'\n",
    "    response = requests.get(base_url + '/article/sjtongzhigg', headers=headers)\n",
    "    if (response.status_code != 200):\n",
    "        return None\n",
    "    text = str(response.content, 'utf-8')\n",
    "    html = etree.HTML(text)\n",
    "    xpath = '//*[@id=\"main\"]/div[2]/div/ul/li/a/text()'\n",
    "    titles = html.xpath(xpath)\n",
    "    xpath = '//*[@id=\"main\"]/div[2]/div/ul/li/a/@href'\n",
    "    links = html.xpath(xpath)\n",
    "    links = [ base_url + str(x) for x in links]\n",
    "    xpath = '//*[@id=\"main\"]/div[2]/div/ul/li/span/text()'\n",
    "    dates = html.xpath(xpath)\n",
    "    dates = [x.split()[0] for x in dates]\n",
    "\n",
    "    data_zhejiang = [ (t, l, d, 'zhejiang') for t, l, d in zip(titles, links, dates) ]\n",
    "    \n",
    "    dict_zhejiang = [{'pos':x[3], 'title': x[0], 'link': x[1], 'date': x[2]} for x in data_zhejiang]\n",
    "    \n",
    "    print (len(dict_zhejiang))\n",
    "\n",
    "    print ('In {}, end, date is {}'.format(get_swb_zhejiang.__name__, time.strftime('%Y-%m-%d-%H:%M', time.localtime(time.time()))))\n",
    "\n",
    "    return dict_zhejiang"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GuangDong"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-03T17:32:10.282057Z",
     "start_time": "2018-08-03T17:32:10.264332Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_swb_guangdong():\n",
    "    \n",
    "    print ('In {}, begin, date is {}'.format(get_swb_guangdong.__name__, time.strftime('%Y-%m-%d-%H:%M', time.localtime(time.time()))))\n",
    "\n",
    "    base_url = 'http://www.gdcom.gov.cn/zwgk/zcwj/'\n",
    "    response = requests.get(base_url, headers=headers)\n",
    "    if (response.status_code != 200):\n",
    "        return None\n",
    "\n",
    "    text = str(response.content, 'utf-8')\n",
    "    html = etree.HTML(text)\n",
    "    xpath = '/html/body/div[2]/div/div[2]/ul/li/a/text()'\n",
    "    titles = html.xpath(xpath)\n",
    "    xpath = '/html/body/div[2]/div/div[2]/ul/li/a/@href'\n",
    "    links = html.xpath(xpath)\n",
    "    tlinks = []\n",
    "    for x in links:\n",
    "        x = str(x)\n",
    "        if x != None and x[:2] == './':\n",
    "            x = base_url + x[2:]\n",
    "        tlinks.append(x)\n",
    "    links = tlinks\n",
    "    \n",
    "    xpath = '/html/body/div[2]/div/div[2]/ul/li/span/text()'\n",
    "    dates = html.xpath(xpath)\n",
    "    dates = [x.replace(' ', '') for x in dates]\n",
    "    \n",
    "    data_guangdong = [ (t, l, d, 'guangdong') for t, l, d in zip(titles, links, dates) ]\n",
    "        \n",
    "    dict_guangdong = [{'pos':x[3], 'title': x[0], 'link': x[1], 'date': x[2]} for x in data_guangdong]\n",
    "    \n",
    "    print (len(dict_guangdong))\n",
    "\n",
    "    print ('In {}, end, date is {}'.format(get_swb_guangdong.__name__, time.strftime('%Y-%m-%d-%H:%M', time.localtime(time.time()))))\n",
    "\n",
    "    return dict_guangdong"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Mysql"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mysql Template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-04T11:33:32.290392Z",
     "start_time": "2018-08-04T11:33:32.236984Z"
    }
   },
   "outputs": [],
   "source": [
    "import pymysql\n",
    "# db = pymysql.connect(host='119.29.190.115', user='impulse', password='njuacmicpc', port=3306)\n",
    "# cursor = db.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-06T17:25:52.909419Z",
     "start_time": "2018-08-06T17:25:52.899126Z"
    }
   },
   "outputs": [],
   "source": [
    "def create_db(cursor, name):\n",
    "    sql = 'create database {name} default character set utf8'.format(name=name)\n",
    "    cursor.execute(sql)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-06T17:25:51.459640Z",
     "start_time": "2018-08-06T17:25:51.449710Z"
    }
   },
   "outputs": [],
   "source": [
    "# # Create table!!!\n",
    "# import pymysql\n",
    "# db = pymysql.connect(host='119.29.190.115', user='impulse', password='njuacmicpc', port=3306, db='gov')\n",
    "# cursor = db.cursor()\n",
    "# sql = 'create table if not exists nyj (pos VARCHAR(32), title VARCHAR(512), link VARCHAR(512), date VARCHAR(64), PRIMARY KEY (link))'\n",
    "# cursor.execute(sql)\n",
    "# db.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-06T17:25:50.036435Z",
     "start_time": "2018-08-06T17:25:50.026708Z"
    }
   },
   "outputs": [],
   "source": [
    "def sql_insert(db, cursor, table, data):\n",
    "    keys = ', '.join(data.keys())\n",
    "    values = ', '.join(['%s'] * len(data))\n",
    "    sql = 'insert into {table}({keys}) values ({values}) on duplicate key update'.format(table=table, \n",
    "            keys=keys, values=values)\n",
    "    update = ','.join([\" {key} = %s\".format(key=key) for key in data])\n",
    "    sql += update\n",
    "    #print (sql)\n",
    "    try:\n",
    "        if cursor.execute(sql, tuple(data.values())*2):\n",
    "            print('Successful')\n",
    "            db.commit()\n",
    "        else:\n",
    "            print ('Nothing to do')\n",
    "    except:\n",
    "        print ('Failed')\n",
    "        db.rollback()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-06T17:25:48.090758Z",
     "start_time": "2018-08-06T17:25:48.076176Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_swb():\n",
    "    db = pymysql.connect(host='119.29.190.115', user='impulse', password='njuacmicpc', port=3306, db='gov')\n",
    "    cursor = db.cursor()\n",
    "    for dic in get_swb_china():\n",
    "        sql_insert(db=db,cursor=cursor,data=dic,table='swb')\n",
    "    for dic in get_swb_jiangsu():\n",
    "        sql_insert(db=db,cursor=cursor,data=dic,table='swb')\n",
    "    for dic in get_swb_guangdong():\n",
    "        sql_insert(db=db,cursor=cursor,data=dic,table='swb')\n",
    "    for dic in get_swb_zhejiang():\n",
    "        sql_insert(db=db,cursor=cursor,data=dic,table='swb')\n",
    "    db.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ShangWuBuWaiMaoSi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-06T17:25:46.773419Z",
     "start_time": "2018-08-06T17:25:46.718607Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_wms_china():\n",
    "    \n",
    "    print ('In {}, begin, date is {}'.format(get_wms_china.__name__, time.strftime('%Y-%m-%d-%H:%M', time.localtime(time.time()))))\n",
    "\n",
    "    base_url = 'http://wms.mofcom.gov.cn'\n",
    "    response = requests.get(base_url + '/article/zcfb/ax/', headers=headers)\n",
    "    if (response.status_code != 200):\n",
    "        return None\n",
    "    text = str(response.content, 'utf-8')\n",
    "    html = etree.HTML(text)\n",
    "    xpath = '//*[@id=\"leftList\"]/div[2]/ul/li/a[1]/text()'\n",
    "    titles = html.xpath(xpath)\n",
    "    # print (titles)\n",
    "    xpath = '//*[@id=\"leftList\"]/div[2]/ul/li/a[1]/@href'\n",
    "    links = html.xpath(xpath)\n",
    "    links = [ base_url + str(x) for x in links]\n",
    "    # print (links)\n",
    "    xpath = '//*[@id=\"leftList\"]/div[2]/ul/li/span/text()'\n",
    "    dates = html.xpath(xpath)\n",
    "    dates = [x.split()[0] for x in dates]\n",
    "    # print (dates)\n",
    "\n",
    "    data_china = [ (t, l, d, 'china') for (t, l, d) in zip(titles, links, dates) ]\n",
    "\n",
    "    response = requests.get(base_url + '/article/zcfb/g/', headers=headers)\n",
    "    if (response.status_code != 200):\n",
    "        return None\n",
    "    text = str(response.content, 'utf-8')\n",
    "    html = etree.HTML(text)\n",
    "    xpath = '//*[@id=\"leftList\"]/div[2]/ul/li/a[1]/text()'\n",
    "    titles = html.xpath(xpath)\n",
    "    xpath = '//*[@id=\"leftList\"]/div[2]/ul/li/a[1]/@href'\n",
    "    links = html.xpath(xpath)\n",
    "    links = [ base_url + str(x) for x in links]\n",
    "    xpath = '//*[@id=\"leftList\"]/div[2]/ul/li/span/text()'\n",
    "    dates = html.xpath(xpath)\n",
    "    dates = [x.split()[0] for x in dates]\n",
    "    \n",
    "    data_china = data_china + [ (t, l, d, 'china') for t, l, d in zip(titles, links, dates) ]\n",
    "\n",
    "    dict_china = [{'pos':x[3], 'title': x[0], 'link': x[1], 'date': x[2]} for x in data_china]\n",
    "  \n",
    "    print (len(dict_china))\n",
    "\n",
    "    print ('In {}, end, date is {}'.format(get_wms_china.__name__, time.strftime('%Y-%m-%d-%H:%M', time.localtime(time.time()))))\n",
    "\n",
    "    return dict_china"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-06T17:25:46.130286Z",
     "start_time": "2018-08-06T17:25:46.108141Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_wms_jiangsu():\n",
    "\n",
    "    print ('In {}, begin, date is {}'.format(get_wms_jiangsu.__name__, time.strftime('%Y-%m-%d-%H:%M', time.localtime(time.time()))))\n",
    "\n",
    "    base_url = 'http://swt.jiangsu.gov.cn'\n",
    "    \n",
    "    response = requests.get(base_url + '/col/col57691/index.html', headers=headers)\n",
    "    if (response.status_code != 200):\n",
    "        return None\n",
    "    text = str(response.content, 'utf-8')\n",
    "    regex = re.compile('<a href=\"(.*)\" target=\"_blank\" .*title=\"(.*)\">.*</a><span.*> \\((.*)\\)</span>')\n",
    "    data_jiangsu = [(x[1], base_url+x[0], x[2].replace('/', '-'), 'jiangsu') for x in re.findall(pattern=regex, string=text)]\n",
    "    \n",
    "    response = requests.get(base_url + '/col/col57692/index.html', headers=headers)\n",
    "    if (response.status_code != 200):\n",
    "        return None\n",
    "    text = str(response.content, 'utf-8')\n",
    "    regex = re.compile('<a href=\"(.*)\" target=\"_blank\" .*title=\"(.*)\">.*</a><span.*> \\((.*)\\)</span>')\n",
    "    data_jiangsu += [(x[1], base_url+x[0], x[2].replace('/', '-'), 'jiangsu') for x in re.findall(pattern=regex, string=text)]\n",
    "    \n",
    "    dict_jiangsu = [{'pos':x[3], 'title': x[0], 'link': x[1], 'date': x[2]} for x in data_jiangsu]\n",
    "    \n",
    "    print (len(dict_jiangsu))\n",
    "\n",
    "    print ('In {}, end, date is {}'.format(get_wms_jiangsu.__name__, time.strftime('%Y-%m-%d-%H:%M', time.localtime(time.time()))))\n",
    "\n",
    "    return dict_jiangsu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-06T17:25:45.416100Z",
     "start_time": "2018-08-06T17:25:45.383436Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_wms_zhejiang():\n",
    "\n",
    "    print ('In {}, begin, date is {}'.format(get_wms_zhejiang.__name__, time.strftime('%Y-%m-%d-%H:%M', time.localtime(time.time()))))\n",
    "\n",
    "    base_url = 'http://www.zcom.gov.cn'\n",
    "    \n",
    "    response = requests.get(base_url + '/col/col1385815/index.html', headers=headers)\n",
    "    if (response.status_code != 200):\n",
    "        return None\n",
    "    text = str(response.content, 'utf-8')\n",
    "\n",
    "    html = etree.HTML(text)\n",
    "    xpath = '//*[@id=\"con_two_2\"]/div[position()>1]/a[1]/text()'\n",
    "    titles = html.xpath(xpath)  \n",
    "    xpath = '//*[@id=\"con_two_2\"]/div[position()>1]/a[1]/@href'\n",
    "    links = html.xpath(xpath)\n",
    "    links = [ base_url + str(x) for x in links]\n",
    "    \n",
    "    xpath = '//*[@id=\"con_two_2\"]/div[position()>1]/a[1]/span/text()'\n",
    "    dates = html.xpath(xpath)\n",
    "    data_zhejiang = [ (t, l, d, 'zhejiang') for t, l, d in zip(titles, links, dates) ]\n",
    "\n",
    "    ################\n",
    "    \n",
    "    xpath = '//*[@id=\"con_three_1\"]/div[position()>1]/a[1]/text()'\n",
    "    titles = html.xpath(xpath)  \n",
    "    xpath = '//*[@id=\"con_three_1\"]/div[position()>1]/a[1]/@href'\n",
    "    links = html.xpath(xpath)\n",
    "    links = [ base_url + str(x) for x in links]\n",
    "    \n",
    "    xpath = '//*[@id=\"con_three_1\"]/div[position()>1]/a[1]/span/text()'\n",
    "    dates = html.xpath(xpath)\n",
    "    data_zhejiang += [ (t, l, d, 'zhejiang') for t, l, d in zip(titles, links, dates) ]\n",
    "    \n",
    "    dict_zhejiang = [{'pos':x[3], 'title': x[0], 'link': x[1], 'date': x[2]} for x in data_zhejiang]\n",
    "  \n",
    "    print (len(dict_zhejiang))\n",
    "\n",
    "    print ('In {}, end, date is {}'.format(get_wms_zhejiang.__name__, time.strftime('%Y-%m-%d-%H:%M', time.localtime(time.time()))))\n",
    "\n",
    "    return dict_zhejiang"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-06T17:25:44.774420Z",
     "start_time": "2018-08-06T17:25:44.768621Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_wms_guangdong():\n",
    "    \n",
    "    print ('In {}, begin, date is {}'.format(get_wms_guangdong.__name__, time.strftime('%Y-%m-%d-%H:%M', time.localtime(time.time()))))\n",
    "\n",
    "    base_url = 'http://go.gdcom.gov.cn'\n",
    "    \n",
    "    response = requests.get(base_url + '/article.php?typeid=9', headers=headers)\n",
    "    if (response.status_code != 200):\n",
    "        return None\n",
    "    text = str(response.content, 'utf-8')\n",
    "\n",
    "    html = etree.HTML(text)\n",
    "    xpath = '/html/body/div[3]/div/div[2]/div/div[2]/div[2]/ul/li/h6/a[1]/text()'\n",
    "    titles = html.xpath(xpath)  \n",
    "    xpath = '/html/body/div[3]/div/div[2]/div/div[2]/div[2]/ul/li/h6/a[1]/@href'\n",
    "    links = html.xpath(xpath)\n",
    "    links = [ base_url + '/' + str(x) for x in links]\n",
    "    \n",
    "    xpath = '/html/body/div[3]/div/div[2]/div/div[2]/div[2]/ul/li/h6/small/text()'\n",
    "    dates = html.xpath(xpath)\n",
    "    data_guangdong = [ (t, l, d, 'guangdong') for t, l, d in zip(titles, links, dates) ]\n",
    "\n",
    "    dict_guangdong = [{'pos':x[3], 'title': x[0], 'link': x[1], 'date': x[2]} for x in data_guangdong]\n",
    "  \n",
    "    print (len(dict_guangdong))\n",
    "\n",
    "    print ('In {}, end, date is {}'.format(get_wms_guangdong.__name__, time.strftime('%Y-%m-%d-%H:%M', time.localtime(time.time()))))\n",
    "\n",
    "    return dict_guangdong"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-06T17:25:44.167457Z",
     "start_time": "2018-08-06T17:25:44.149982Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_wms():\n",
    "    db = pymysql.connect(host='119.29.190.115', user='impulse', password='njuacmicpc', port=3306, db='gov')\n",
    "    cursor = db.cursor()\n",
    "    for dic in get_wms_china():\n",
    "        sql_insert(db=db,cursor=cursor,data=dic,table='wms')\n",
    "    for dic in get_wms_jiangsu():\n",
    "        sql_insert(db=db,cursor=cursor,data=dic,table='wms')\n",
    "    for dic in get_wms_guangdong():\n",
    "        sql_insert(db=db,cursor=cursor,data=dic,table='wms')\n",
    "    for dic in get_wms_zhejiang():\n",
    "        sql_insert(db=db,cursor=cursor,data=dic,table='wms')\n",
    "    db.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ZhiShiChanQuanJu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-06T17:25:42.774696Z",
     "start_time": "2018-08-06T17:25:42.719937Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_zscqj_china():\n",
    "    \n",
    "    print ('In {}, begin, date is {}'.format(get_zscqj_china.__name__, time.strftime('%Y-%m-%d-%H:%M', time.localtime(time.time()))))\n",
    "\n",
    "    base_url = 'http://www.sipo.gov.cn'\n",
    "    \n",
    "    response = requests.get(base_url + '/gwywj/index.htm', headers=headers)   \n",
    "    if (response.status_code != 200):\n",
    "        return None\n",
    "    text = str(response.content, 'utf-8')\n",
    "    html = etree.HTML(text)\n",
    "    xpath = '/html/body/div/div/div/div[4]/div/div[2]/ul/li/a[1]/text()'\n",
    "    titles = html.xpath(xpath)\n",
    "    # print (titles)\n",
    "    xpath = '/html/body/div/div/div/div[4]/div/div[2]/ul/li/a[1]/@href'\n",
    "    links = html.xpath(xpath)\n",
    "    # print (links)\n",
    "    xpath = '/html/body/div/div/div/div[4]/div/div[2]/ul/li/span/text()'\n",
    "    dates = html.xpath(xpath)\n",
    "    # print (dates)\n",
    "    data_china = [ (t, l, d, 'china') for (t, l, d) in zip(titles, links, dates) ]\n",
    "\n",
    "    response = requests.get(base_url + '/dtxx/index.htm', headers=headers)\n",
    "    if (response.status_code != 200):\n",
    "        return None\n",
    "    text = str(response.content, 'utf-8')\n",
    "    html = etree.HTML(text)\n",
    "    xpath = '/html/body/div/div/div/div[4]/div/div[2]/ul/li/a[1]/text()'\n",
    "    titles = html.xpath(xpath)\n",
    "    # print (titles)\n",
    "    xpath = '/html/body/div/div/div/div[4]/div/div[2]/ul/li/a[1]/@href'\n",
    "    links = html.xpath(xpath)\n",
    "    ## !!!!!\n",
    "    links = [ base_url + '/dtxx/' + str(x) for x in links] \n",
    "    # print (links)\n",
    "    xpath = '/html/body/div/div/div/div[4]/div/div[2]/ul/li/span/text()'\n",
    "    dates = html.xpath(xpath)\n",
    "    # print (dates) \n",
    "    data_china = data_china + [ (t, l, d, 'china') for t, l, d in zip(titles, links, dates) ]\n",
    "\n",
    "    response = requests.get(base_url + '/zfgg/index.htm', headers=headers)\n",
    "    if (response.status_code != 200):\n",
    "        return None\n",
    "    text = str(response.content, 'utf-8')\n",
    "    html = etree.HTML(text)\n",
    "    xpath = '/html/body/div/div/div/div[4]/div/div[2]/ul/li/a[1]/text()'\n",
    "    titles = html.xpath(xpath)\n",
    "    # print (titles)\n",
    "    xpath = '/html/body/div/div/div/div[4]/div/div[2]/ul/li/a[1]/@href'\n",
    "    links = html.xpath(xpath)\n",
    "    ## !!!!!\n",
    "    links = [ base_url + '/zfgg/' + str(x) for x in links]\n",
    "    # print (links)\n",
    "    xpath = '/html/body/div/div/div/div[4]/div/div[2]/ul/li/span/text()'\n",
    "    dates = html.xpath(xpath)\n",
    "    # print (dates) \n",
    "    data_china = data_china + [ (t, l, d, 'china') for t, l, d in zip(titles, links, dates) ]\n",
    "    \n",
    "    response = requests.get(base_url + '/gztz/index.htm', headers=headers)\n",
    "    if (response.status_code != 200):\n",
    "        return None\n",
    "    text = str(response.content, 'utf-8')\n",
    "    html = etree.HTML(text)\n",
    "    xpath = '/html/body/div/div/div/div[4]/div/div[2]/ul/li/a[1]/text()'\n",
    "    titles = html.xpath(xpath)\n",
    "    # print (titles)\n",
    "    xpath = '/html/body/div/div/div/div[4]/div/div[2]/ul/li/a[1]/@href'\n",
    "    links = html.xpath(xpath)\n",
    "    ## !!!!!\n",
    "    links = [ base_url + '/gztz/' + str(x) for x in links]\n",
    "    # print (links)\n",
    "    xpath = '/html/body/div/div/div/div[4]/div/div[2]/ul/li/span/text()'\n",
    "    dates = html.xpath(xpath)\n",
    "    # print (dates) \n",
    "    data_china = data_china + [ (t, l, d, 'china') for t, l, d in zip(titles, links, dates) ]\n",
    "    \n",
    "    \n",
    "    dict_china = [{'pos':x[3], 'title': x[0], 'link': x[1], 'date': x[2]} for x in data_china]\n",
    "  \n",
    "    print (len(dict_china))\n",
    "\n",
    "    print ('In {}, end, date is {}'.format(get_zscqj_china.__name__, time.strftime('%Y-%m-%d-%H:%M', time.localtime(time.time()))))\n",
    "\n",
    "    return dict_china"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-06T17:25:41.967096Z",
     "start_time": "2018-08-06T17:25:41.933515Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_zscqj_jiangsu():\n",
    "\n",
    "    print ('In {}, begin, date is {}'.format(get_zscqj_jiangsu.__name__, time.strftime('%Y-%m-%d-%H:%M', time.localtime(time.time()))))\n",
    "\n",
    "    base_url = 'http://jsip.jiangsu.gov.cn'\n",
    "    \n",
    "    response = requests.get(base_url + '/col/col3300/index.html', headers=headers)\n",
    "    if (response.status_code != 200):\n",
    "        return None\n",
    "    text = str(response.content, 'utf-8')\n",
    "    regex = re.compile('<a href=\"(.*)\" TARGET=\"_blank\" >.*<div class=\"text\">(.*)</div>.*<div class=\"text-date\">(.*)</div>.*</a>')\n",
    "    data_jiangsu = [(x[1], base_url+x[0], x[2], 'jiangsu') for x in re.findall(pattern=regex, string=text)]\n",
    "    \n",
    "    response = requests.get(base_url + '/col/col3252/index.html', headers=headers)\n",
    "    if (response.status_code != 200):\n",
    "        return None\n",
    "    text = str(response.content, 'utf-8')\n",
    "    regex = re.compile('<a href=\"(.*)\" TARGET=\"_blank\" >.*<div class=\"text\">(.*)</div>.*<div class=\"text-date\">(.*)</div>.*</a>')\n",
    "    data_jiangsu += [(x[1], base_url+x[0], x[2], 'jiangsu') for x in re.findall(pattern=regex, string=text)]\n",
    "    \n",
    "    dict_jiangsu = [{'pos':x[3], 'title': x[0], 'link': x[1], 'date': x[2]} for x in data_jiangsu]\n",
    "    \n",
    "    print (len(dict_jiangsu))\n",
    "\n",
    "    print ('In {}, end, date is {}'.format(get_zscqj_jiangsu.__name__, time.strftime('%Y-%m-%d-%H:%M', time.localtime(time.time()))))\n",
    "\n",
    "    return dict_jiangsu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-06T17:25:41.271826Z",
     "start_time": "2018-08-06T17:25:41.239311Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_zscqj_zhejiang():\n",
    "\n",
    "    print ('In {}, begin, date is {}'.format(get_zscqj_zhejiang.__name__, time.strftime('%Y-%m-%d-%H:%M', time.localtime(time.time()))))\n",
    "\n",
    "    base_url = 'http://www.zjpat.gov.cn'\n",
    "    \n",
    "    response = requests.get(base_url + '/interIndex.do?method=list22&dir=/zjszscqj/tzgg', headers=headers)\n",
    "    if (response.status_code != 200):\n",
    "        return None\n",
    "    text = str(response.content, 'utf-8').replace('\\r\\n', '')\n",
    "    text = re.sub(' +', ' ', text)\n",
    "\n",
    "    regex = re.compile('<a href=\"([^>]*?)\" title=\"([^>]*?)\" target=\"_blank\" class=\"color_01\">.*?</a> </td> <td.[^>]*?> \\((.*?)\\) </td> </tr>')\n",
    "    data_zhejiang = [(x[1], base_url+'/'+x[0], x[2], 'zhejiang') for x in re.findall(pattern=regex, string=text)]\n",
    "    \n",
    "    response = requests.get(base_url + '/interIndex.do?method=list2&dir=/zjszscqj/xwdt/sxdt', headers=headers)\n",
    "    if (response.status_code != 200):\n",
    "        return None\n",
    "    text = str(response.content, 'utf-8').replace('\\r\\n', '')\n",
    "    text = re.sub(' +', ' ', text)\n",
    "    regex = re.compile('<a href=\"([^>]*?)\" title=\"([^>]*?)\" target=\"_blank\" class=\"color_01\">.*?</a> </td> <td.[^>]*?> \\((.*?)\\) </td> </tr>')\n",
    "    data_zhejiang += [(x[1], base_url+'/'+x[0], x[2], 'zhejiang') for x in re.findall(pattern=regex, string=text)]\n",
    "    \n",
    "    dict_zhejiang = [{'pos':x[3], 'title': x[0], 'link': x[1], 'date': x[2]} for x in data_zhejiang]\n",
    "    \n",
    "    print (len(dict_zhejiang))\n",
    "    \n",
    "    print ('In {}, end, date is {}'.format(get_zscqj_zhejiang.__name__, time.strftime('%Y-%m-%d-%H:%M', time.localtime(time.time()))))\n",
    "\n",
    "    return dict_zhejiang\n",
    "\n",
    "### Very/interIndex.do?method=list2&dir=/zjszscqj/xwdt/sxdt Important to match \\r\\n...\n",
    "### re.findall(pattern='<table>([\\s\\S]*?)<\\/table>', string=text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-06T17:25:40.548316Z",
     "start_time": "2018-08-06T17:25:40.523379Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_zscqj_guangdong():\n",
    "\n",
    "    print ('In {}, begin, date is {}'.format(get_zscqj_guangdong.__name__, time.strftime('%Y-%m-%d-%H:%M', time.localtime(time.time()))))\n",
    "\n",
    "    base_url = 'http://www.gdipo.gov.cn'\n",
    "    \n",
    "    response = requests.get(base_url + '/gdipo/gdipodt/list.shtml', headers=headers)\n",
    "    if (response.status_code != 200):\n",
    "        return None\n",
    "    text = str(response.content, 'utf-8').replace('\\r\\n', '')\n",
    "    text = re.sub(' +', ' ', text)\n",
    "    regex = re.compile('<li> <a href=\"(.*?)\" target=\"_blank\" >(.*?)</a> <div class=\"time_div\"> <span class=\"time\">(.*?)</span> </div> </li>')\n",
    "    data_guangdong = [(x[1], base_url+x[0], x[2], 'guangdong') for x in re.findall(pattern=regex, string=text)]\n",
    "\n",
    "    response = requests.get(base_url + '/gdipo/tzgg/list.shtml', headers=headers)\n",
    "    if (response.status_code != 200):\n",
    "        return None\n",
    "    text = str(response.content, 'utf-8').replace('\\r\\n', '')\n",
    "    text = re.sub(' +', ' ', text)\n",
    "    regex = re.compile('<li> <a href=\"(.*?)\" target=\"_blank\" >(.*?)</a> <div class=\"time_div\"> <span class=\"time\">(.*?)</span> </div> </li>')\n",
    "    data_guangdong += [(x[1], base_url+x[0], x[2], 'guangdong') for x in re.findall(pattern=regex, string=text)]\n",
    "   \n",
    "    dict_guangdong = [{'pos':x[3], 'title': x[0], 'link': x[1], 'date': x[2]} for x in data_guangdong]\n",
    "    \n",
    "    print (len(dict_guangdong))\n",
    "    \n",
    "    print ('In {}, end, date is {}'.format(get_zscqj_guangdong.__name__, time.strftime('%Y-%m-%d-%H:%M', time.localtime(time.time()))))\n",
    "\n",
    "    return dict_guangdong\n",
    "\n",
    "### Very/interIndex.do?method=list2&dir=/zjszscqj/xwdt/sxdt Important to match \\r\\n...\n",
    "### re.findall(pattern='<table>([\\s\\S]*?)<\\/table>', string=text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-06T17:25:39.740555Z",
     "start_time": "2018-08-06T17:25:39.732540Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_zscqj():\n",
    "    db = pymysql.connect(host='119.29.190.115', user='impulse', password='njuacmicpc', port=3306, db='gov')\n",
    "    cursor = db.cursor()\n",
    "    for dic in get_zscqj_china():\n",
    "        sql_insert(db=db,cursor=cursor,data=dic,table='zscqj')\n",
    "    for dic in get_zscqj_jiangsu():\n",
    "        sql_insert(db=db,cursor=cursor,data=dic,table='zscqj')\n",
    "    for dic in get_zscqj_guangdong():\n",
    "        sql_insert(db=db,cursor=cursor,data=dic,table='zscqj')\n",
    "    for dic in get_zscqj_zhejiang():\n",
    "        sql_insert(db=db,cursor=cursor,data=dic,table='zscqj')\n",
    "    db.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-03T17:38:53.331658Z",
     "start_time": "2018-08-03T17:38:53.327137Z"
    }
   },
   "source": [
    "## NengYuanJu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-06T17:25:36.546812Z",
     "start_time": "2018-08-06T17:25:36.523106Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_nyj_china():\n",
    "    print ('In {}, begin, date is {}'.format(get_zscqj_china.__name__, time.strftime('%Y-%m-%d-%H:%M', time.localtime(time.time()))))\n",
    "\n",
    "    base_url = 'http://www.nea.gov.cn'\n",
    "    \n",
    "    response = requests.get(base_url + '/xwzx/nyyw.htm', headers=headers)   \n",
    "    if (response.status_code != 200):\n",
    "        return None\n",
    "    text = str(response.content, 'utf-8')\n",
    "    html = etree.HTML(text)\n",
    "    xpath = '//div[@class=\"content\"]/div/ul/li/a/text()'\n",
    "    titles = html.xpath(xpath)\n",
    "    # print (titles)\n",
    "    xpath = '//div[@class=\"content\"]/div/ul/li/a/@href'\n",
    "    links = html.xpath(xpath)\n",
    "    # print (links)\n",
    "    xpath = '//div[@class=\"content\"]/div/ul/li/span/text()'\n",
    "    dates = html.xpath(xpath)\n",
    "    # print (dates)\n",
    "    data_china = [ (t, l, d, 'china') for (t, l, d) in zip(titles, links, dates) ]\n",
    "    \n",
    "    dict_china = [{'pos':x[3], 'title': x[0], 'link': x[1], 'date': x[2]} for x in data_china]\n",
    "  \n",
    "    print (len(dict_china))\n",
    "    \n",
    "    print ('In {}, end, date is {}'.format(get_zscqj_china.__name__, time.strftime('%Y-%m-%d-%H:%M', time.localtime(time.time()))))\n",
    "\n",
    "    return dict_china"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-06T17:25:35.900770Z",
     "start_time": "2018-08-06T17:25:35.884576Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_nyj_jiangsu():\n",
    "    print ('In {}, begin, date is {}'.format(get_zscqj_jiangsu.__name__, time.strftime('%Y-%m-%d-%H:%M', time.localtime(time.time()))))\n",
    "\n",
    "    base_url = 'http://jsb.nea.gov.cn'\n",
    "    \n",
    "    response = requests.get(base_url + '/info/community/101.html', headers=headers)   \n",
    "    if (response.status_code != 200):\n",
    "        return None\n",
    "    text = str(response.content, 'gbk').replace('\\r\\n', '')\n",
    "    text = re.sub(' +', ' ', text)\n",
    "    html = etree.HTML(text)\n",
    "\n",
    "    regex = re.compile('<a href=\"([^>]*?)\" target=_blank>(.*?)</a></td><td.*?> <p.*?>\\[(.*?)\\] </td>')\n",
    "    data_jiangsu = [(x[1], base_url+x[0], x[2], 'jiangsu') for x in re.findall(pattern=regex, string=text)]\n",
    "    \n",
    "    dict_jiangsu = [{'pos':x[3], 'title': x[0], 'link': x[1], 'date': x[2].replace('/', '-')} for x in data_jiangsu]\n",
    "    \n",
    "    print (len(dict_jiangsu))\n",
    "    \n",
    "    print ('In {}, end, date is {}'.format(get_zscqj_jiangsu.__name__, time.strftime('%Y-%m-%d-%H:%M', time.localtime(time.time()))))\n",
    "\n",
    "    return dict_jiangsu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-06T17:25:34.611402Z",
     "start_time": "2018-08-06T17:25:34.586838Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_nyj_zhejiang():\n",
    "    print ('In {}, begin, date is {}'.format(get_zscqj_zhejiang.__name__, time.strftime('%Y-%m-%d-%H:%M', time.localtime(time.time()))))\n",
    "\n",
    "    base_url = 'http://zjb.nea.gov.cn'\n",
    "\n",
    "    response = requests.get(base_url + '/article/zygg/d1/', headers=headers)   \n",
    "    if (response.status_code != 200):\n",
    "        return None\n",
    "    text = str(response.content, 'gbk')\n",
    "    html = etree.HTML(text)\n",
    "    xpath = '//div[@class=\"d5\"]/a/text()'\n",
    "    titles = html.xpath(xpath)\n",
    "    # print (titles)\n",
    "    xpath = '//div[@class=\"d5\"]/a/@href'\n",
    "    links = html.xpath(xpath)\n",
    "    links = [base_url + x for x in links]\n",
    "    # print (links)\n",
    "    xpath = '//div[@class=\"d5\"]/span/text()'\n",
    "    dates = html.xpath(xpath)\n",
    "    # print (dates)\n",
    "    data_zhejiang = [ (t, l, d, 'zhejiang') for (t, l, d) in zip(titles, links, dates) ]\n",
    "    \n",
    "    response = requests.get(base_url + '/article/ywdd/', headers=headers)   \n",
    "    if (response.status_code != 200):\n",
    "        return None\n",
    "    text = str(response.content, 'gbk')\n",
    "    html = etree.HTML(text)\n",
    "    xpath = '//div[@class=\"d5\"]/a/text()'\n",
    "    titles = html.xpath(xpath)\n",
    "    # print (titles)\n",
    "    xpath = '//div[@class=\"d5\"]/a/@href'\n",
    "    links = html.xpath(xpath)\n",
    "    links = [base_url + x for x in links]\n",
    "    # print (links)\n",
    "    xpath = '//div[@class=\"d5\"]/span/text()'\n",
    "    dates = html.xpath(xpath)\n",
    "    # print (dates)\n",
    "    data_zhejiang += [ (t, l, d, 'zhejiang') for (t, l, d) in zip(titles, links, dates) ]\n",
    "    \n",
    "    dict_zhejiang = [{'pos':x[3], 'title': x[0], 'link': x[1], 'date': x[2]} for x in data_zhejiang]\n",
    "  \n",
    "    print (len(dict_zhejiang))\n",
    "    \n",
    "    print ('In {}, end, date is {}'.format(get_zscqj_zhejiang.__name__, time.strftime('%Y-%m-%d-%H:%M', time.localtime(time.time()))))\n",
    "\n",
    "    return dict_zhejiang"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-06T17:25:33.581106Z",
     "start_time": "2018-08-06T17:25:33.561880Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_nyj_guangdong():\n",
    "    ## Infact, it is nyj of southern china\n",
    "    print ('In {}, begin, date is {}'.format(get_nyj_guangdong.__name__, time.strftime('%Y-%m-%d-%H:%M', time.localtime(time.time()))))\n",
    "\n",
    "    base_url = 'http://nfj.nea.gov.cn'\n",
    "    \n",
    "    response = requests.get(base_url + '/frontIndex/showNews.do?type=3', headers=headers)   \n",
    "    if (response.status_code != 200):\n",
    "        return None\n",
    "    text = str(response.content, 'utf-8')\n",
    "    html = etree.HTML(text)\n",
    "    xpath = '//div[@class=\"new_list2\"]/ul/li/a/text()'\n",
    "    titles = html.xpath(xpath)\n",
    "    # print (titles)\n",
    "    xpath = '//div[@class=\"new_list2\"]/ul/li/a/@href'\n",
    "    links = html.xpath(xpath)\n",
    "    # print (links)\n",
    "    xpath = '//div[@class=\"new_list2\"]/ul/li/span/text()'\n",
    "    dates = html.xpath(xpath)\n",
    "    # print (dates)\n",
    "    data_guangdong = [ (t, l, d, 'guangdong') for (t, l, d) in zip(titles, links, dates) ]\n",
    "    \n",
    "    dict_guangdong = [{'pos':x[3], 'title': x[0], 'link': x[1], 'date': x[2]} for x in data_guangdong]\n",
    "    \n",
    "    print (len(dict_guangdong))\n",
    "\n",
    "    print ('In {}, end, date is {}'.format(get_nyj_guangdong.__name__, time.strftime('%Y-%m-%d-%H:%M', time.localtime(time.time()))))\n",
    "\n",
    "    return dict_guangdong"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-06T17:25:32.613119Z",
     "start_time": "2018-08-06T17:25:32.600930Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_nyj():\n",
    "    db = pymysql.connect(host='119.29.190.115', user='impulse', password='njuacmicpc', port=3306, db='gov')\n",
    "    cursor = db.cursor()\n",
    "    for dic in get_nyj_china():\n",
    "        sql_insert(db=db,cursor=cursor,data=dic,table='nyj')\n",
    "    for dic in get_nyj_jiangsu():\n",
    "        sql_insert(db=db,cursor=cursor,data=dic,table='nyj')\n",
    "    for dic in get_nyj_guangdong():\n",
    "        sql_insert(db=db,cursor=cursor,data=dic,table='nyj')\n",
    "    for dic in get_nyj_zhejiang():\n",
    "        sql_insert(db=db,cursor=cursor,data=dic,table='nyj')\n",
    "    db.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-06T17:25:27.507659Z",
     "start_time": "2018-08-06T17:25:27.489513Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_jbw_china():\n",
    "    print ('In {}, begin, date is {}'.format(get_zscqj_china.__name__, time.strftime('%Y-%m-%d-%H:%M', time.localtime(time.time()))))\n",
    "\n",
    "    base_url = 'http://www.cfstc.org' \n",
    "    \n",
    "    response = requests.get(base_url + '/jinbiaowei/2929484/index.html', headers=headers)\n",
    "    if (response.status_code != 200):\n",
    "        return None\n",
    "    text = str(response.content, 'utf-8')\n",
    "    html = etree.HTML(text)\n",
    "    xpath = '//table[@opentype=\"page\"]/tbody/tr/td/ul/li/a[1]/text()'\n",
    "    titles = html.xpath(xpath)\n",
    "    # print (titles)\n",
    "    xpath = '//table[@opentype=\"page\"]/tbody/tr/td/ul/li/a[1]/@href'\n",
    "    links = html.xpath(xpath)\n",
    "    ## !!!!!\n",
    "    links = [ base_url + '/gztz/' + str(x) for x in links]\n",
    "    # print (links)\n",
    "    xpath = '//table[@opentype=\"page\"]/tbody/tr/td/ul/li/span/text()'\n",
    "    dates = html.xpath(xpath)\n",
    "    # print (dates) \n",
    "    data_china = [ (t, l, d, 'china') for t, l, d in zip(titles, links, dates) ]\n",
    "    \n",
    "    dict_china = [{'pos':x[3], 'title': x[0], 'link': x[1], 'date': x[2]} for x in data_china]\n",
    "  \n",
    "    print (len(dict_china))\n",
    "\n",
    "    print ('In {}, end, date is {}'.format(get_zscqj_china.__name__, time.strftime('%Y-%m-%d-%H:%M', time.localtime(time.time()))))\n",
    "\n",
    "    return dict_china"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-06T17:24:39.764379Z",
     "start_time": "2018-08-06T17:24:39.760686Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_jbw():\n",
    "    db = pymysql.connect(host='119.29.190.115', user='impulse', password='njuacmicpc', port=3306, db='gov')\n",
    "    cursor = db.cursor()\n",
    "    for dic in get_jbw_china():\n",
    "        sql_insert(db=db,cursor=cursor,data=dic,table='jbw')\n",
    "    db.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-06T17:20:56.491867Z",
     "start_time": "2018-08-06T17:20:56.468538Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_zjh_china():\n",
    "    print ('In {}, begin, date is {}'.format(get_zjh_china.__name__, time.strftime('%Y-%m-%d-%H:%M', time.localtime(time.time()))))\n",
    "\n",
    "    from selenium import webdriver\n",
    "    from selenium.webdriver.common.by import By\n",
    "    from selenium.webdriver.support.ui import WebDriverWait\n",
    "    from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "    chrome_options = webdriver.ChromeOptions()\n",
    "    chrome_options.add_argument('--headless')\n",
    "    driver = webdriver.Chrome(chrome_options=chrome_options)\n",
    "\n",
    "    base_url = 'http://www.csrc.gov.cn/pub/zjhpublic' \n",
    "    \n",
    "    driver.get(base_url + '/index.htm?channel=3300/3311/')\n",
    "    driver.switch_to.frame(\"DataList\")\n",
    "    driver.execute_script(\"window.scrollTo(0,document.body.scrollHeight)\")\n",
    "    time.sleep(1)\n",
    "#     html = driver.page_source\n",
    "    elements = driver.find_elements_by_xpath('//div[@class=\"row\"]/li[@class=\"mc\"]/div/a')\n",
    "    urls = [ele.get_attribute(\"href\") for ele in elements if len(ele.text) > 0]\n",
    "    titles = [ele.text for ele in elements if len(ele.text) > 0]\n",
    "    elements = driver.find_elements_by_xpath('//div[@class=\"row\"]/li[@class=\"fbrq\"]')\n",
    "    dates = [ele.text for ele in elements if len(ele.text) > 0]\n",
    "    dates = [ date.replace('年','-').replace('月','-').replace('日','') for date in dates]\n",
    "\n",
    "    data_china = [(x[0], x[1], x[2]) for x in zip(titles,urls,dates)]    \n",
    "    \n",
    "    driver.get(base_url + '/index.htm?channel=3300/3302/')\n",
    "    driver.switch_to.frame(\"DataList\")\n",
    "    driver.execute_script(\"window.scrollTo(0,document.body.scrollHeight)\")\n",
    "    time.sleep(1)\n",
    "#     html = driver.page_source\n",
    "    elements = driver.find_elements_by_xpath('//div[@class=\"row\"]/li[@class=\"mc\"]/div/a')\n",
    "    urls = [ele.get_attribute(\"href\") for ele in elements if len(ele.text) > 0]\n",
    "    titles = [ele.text for ele in elements if len(ele.text) > 0]\n",
    "    elements = driver.find_elements_by_xpath('//div[@class=\"row\"]/li[@class=\"fbrq\"]')\n",
    "    dates = [ele.text for ele in elements if len(ele.text) > 0]\n",
    "    dates = [ date.replace('年','-').replace('月','-').replace('日','') for date in dates]\n",
    "\n",
    "    data_china += [(x[0], x[1], x[2]) for x in zip(titles,urls,dates)]\n",
    "\n",
    "    \n",
    "    driver.quit()    \n",
    "    \n",
    "    dict_china = [{'pos': 'china', 'title': x[0], 'link': x[1], 'date': x[2]} for x in data_china]\n",
    "    \n",
    "    print (len(dict_china))\n",
    "    \n",
    "    print ('In {}, end, date is {}'.format(get_zjh_china.__name__, time.strftime('%Y-%m-%d-%H:%M', time.localtime(time.time()))))\n",
    "\n",
    "    return dict_china"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-06T17:21:01.286597Z",
     "start_time": "2018-08-06T17:21:01.263173Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_zjh_jiangsu():\n",
    "    print ('In {}, begin, date is {}'.format(get_zjh_jiangsu.__name__, time.strftime('%Y-%m-%d-%H:%M', time.localtime(time.time()))))\n",
    "\n",
    "    from selenium import webdriver\n",
    "    from selenium.webdriver.common.by import By\n",
    "    from selenium.webdriver.support.ui import WebDriverWait\n",
    "    from selenium.webdriver.support import expected_conditions as EC\n",
    "    \n",
    "    chrome_options = webdriver.ChromeOptions()\n",
    "    chrome_options.add_argument('--headless')\n",
    "    driver = webdriver.Chrome(chrome_options=chrome_options)\n",
    "\n",
    "    base_url = 'http://www.csrc.gov.cn/pub/zjhpublicofjs' \n",
    "    \n",
    "    driver.get(base_url + '/index.htm?channel=3284/3565/')\n",
    "    driver.switch_to.frame(\"DataList\")\n",
    "    \n",
    "    driver.execute_script(\"window.scrollTo(0,document.body.scrollHeight)\")\n",
    "    time.sleep(1)\n",
    "#     html = driver.page_source\n",
    "    elements = driver.find_elements_by_xpath('//div[@class=\"row\"]/li[@class=\"mc\"]/div/a')\n",
    "    urls = [ele.get_attribute(\"href\") for ele in elements if len(ele.text) > 0 ]\n",
    "    titles = [ele.text for ele in elements if len(ele.text) > 0 ]\n",
    "    elements = driver.find_elements_by_xpath('//div[@class=\"row\"]/li[@class=\"fbrq\"]')\n",
    "    dates = [ele.text for ele in elements if len(ele.text) > 0 ]\n",
    "    dates = [ date.replace('年','-').replace('月','-').replace('日','') for date in dates]\n",
    "\n",
    "    data_jiangsu = [(x[0], x[1], x[2]) for x in zip(titles,urls,dates)]    \n",
    "    \n",
    "    driver.quit()    \n",
    "    \n",
    "    dict_jiangsu = [{'pos': 'jiangsu', 'title': x[0], 'link': x[1], 'date': x[2]} for x in data_jiangsu]\n",
    "    \n",
    "    print (len(dict_jiangsu))\n",
    "    \n",
    "    print ('In {}, end, date is {}'.format(get_zjh_jiangsu.__name__, time.strftime('%Y-%m-%d-%H:%M', time.localtime(time.time()))))\n",
    "\n",
    "    return dict_jiangsu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-06T17:21:57.985339Z",
     "start_time": "2018-08-06T17:21:57.962314Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_zjh_zhejiang():\n",
    "    print ('In {}, begin, date is {}'.format(get_zjh_zhejiang.__name__, time.strftime('%Y-%m-%d-%H:%M', time.localtime(time.time()))))\n",
    "\n",
    "    from selenium import webdriver\n",
    "    from selenium.webdriver.common.by import By\n",
    "    from selenium.webdriver.support.ui import WebDriverWait\n",
    "    from selenium.webdriver.support import expected_conditions as EC\n",
    "    \n",
    "    chrome_options = webdriver.ChromeOptions()\n",
    "    chrome_options.add_argument('--headless')\n",
    "    driver = webdriver.Chrome(chrome_options=chrome_options)\n",
    "\n",
    "    base_url = 'http://www.csrc.gov.cn/pub/zjhpublicofzj' \n",
    "    \n",
    "    driver.get(base_url + '/index.htm?channel=3284/3565/')\n",
    "    driver.switch_to.frame(\"DataList\")\n",
    "    \n",
    "    driver.execute_script(\"window.scrollTo(0,document.body.scrollHeight)\")\n",
    "    time.sleep(1)\n",
    "#     html = driver.page_source\n",
    "    elements = driver.find_elements_by_xpath('//div[@class=\"row\"]/li[@class=\"mc\"]/div/a')\n",
    "    urls = [ele.get_attribute(\"href\") for ele in elements if len(ele.text) > 0 ]\n",
    "    titles = [ele.text for ele in elements if len(ele.text) > 0 ]\n",
    "    elements = driver.find_elements_by_xpath('//div[@class=\"row\"]/li[@class=\"fbrq\"]')\n",
    "    dates = [ele.text for ele in elements if len(ele.text) > 0 ]\n",
    "    dates = [ date.replace('年','-').replace('月','-').replace('日','') for date in dates]\n",
    "\n",
    "    data_zhejiang = [(x[0], x[1], x[2]) for x in zip(titles,urls,dates)]    \n",
    "    \n",
    "    driver.quit()    \n",
    "    \n",
    "    dict_zhejiang = [{'pos': 'zhejiang', 'title': x[0], 'link': x[1], 'date': x[2]} for x in data_zhejiang]\n",
    "    \n",
    "    print (len(dict_zhejiang))\n",
    "    \n",
    "    print ('In {}, end, date is {}'.format(get_zjh_zhejiang.__name__, time.strftime('%Y-%m-%d-%H:%M', time.localtime(time.time()))))\n",
    "\n",
    "    return dict_zhejiang"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-06T17:22:56.165110Z",
     "start_time": "2018-08-06T17:22:56.144581Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_zjh_guangdong():\n",
    "    print ('In {}, begin, date is {}'.format(get_zjh_guangdong.__name__, time.strftime('%Y-%m-%d-%H:%M', time.localtime(time.time()))))\n",
    "\n",
    "    from selenium import webdriver\n",
    "    from selenium.webdriver.common.by import By\n",
    "    from selenium.webdriver.support.ui import WebDriverWait\n",
    "    from selenium.webdriver.support import expected_conditions as EC\n",
    "    \n",
    "    chrome_options = webdriver.ChromeOptions()\n",
    "    chrome_options.add_argument('--headless')\n",
    "    driver = webdriver.Chrome(chrome_options=chrome_options)\n",
    "\n",
    "    base_url = 'http://www.csrc.gov.cn/pub/zjhpublicofgd' \n",
    "    \n",
    "    driver.get(base_url + '/index.htm?channel=3284/3565/')\n",
    "    driver.switch_to.frame(\"DataList\")\n",
    "    \n",
    "    driver.execute_script(\"window.scrollTo(0,document.body.scrollHeight)\")\n",
    "    time.sleep(1)\n",
    "#     html = driver.page_source\n",
    "    elements = driver.find_elements_by_xpath('//div[@class=\"row\"]/li[@class=\"mc\"]/div/a')\n",
    "    urls = [ele.get_attribute(\"href\") for ele in elements if len(ele.text) > 0 ]\n",
    "    titles = [ele.text for ele in elements if len(ele.text) > 0 ]\n",
    "    elements = driver.find_elements_by_xpath('//div[@class=\"row\"]/li[@class=\"fbrq\"]')\n",
    "    dates = [ele.text for ele in elements if len(ele.text) > 0 ]\n",
    "    dates = [ date.replace('年','-').replace('月','-').replace('日','') for date in dates]\n",
    "\n",
    "    data_guangdong = [(x[0], x[1], x[2]) for x in zip(titles,urls,dates)]    \n",
    "    \n",
    "    driver.quit()    \n",
    "    \n",
    "    dict_guangdong = [{'pos': 'guangdong', 'title': x[0], 'link': x[1], 'date': x[2]} for x in data_guangdong]\n",
    "    \n",
    "    print (len(dict_guangdong))\n",
    "    \n",
    "    print ('In {}, end, date is {}'.format(get_zjh_guangdong.__name__, time.strftime('%Y-%m-%d-%H:%M', time.localtime(time.time()))))\n",
    "\n",
    "    return dict_guangdong"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-06T17:28:09.224244Z",
     "start_time": "2018-08-06T17:28:09.209524Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_zjh():\n",
    "    db = pymysql.connect(host='119.29.190.115', user='impulse', password='njuacmicpc', port=3306, db='gov')\n",
    "    cursor = db.cursor()\n",
    "    for dic in get_zjh_china():\n",
    "        sql_insert(db=db,cursor=cursor,data=dic,table='zjh')\n",
    "    for dic in get_zjh_guangdong():\n",
    "        sql_insert(db=db,cursor=cursor,data=dic,table='zjh')\n",
    "    for dic in get_zjh_jiangsu():\n",
    "        sql_insert(db=db,cursor=cursor,data=dic,table='zjh')\n",
    "    for dic in get_zjh_zhejiang():\n",
    "        sql_insert(db=db,cursor=cursor,data=dic,table='zjh')\n",
    "    db.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
